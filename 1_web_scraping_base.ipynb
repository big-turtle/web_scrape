{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping: Almost Data Science With Python And BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping is a wondeful facet of programming which lets you collect incredible amounts of information automatically. Along with the freedom you get from not needing to sit in front of a computer for 6 hours, you're also faced with the legal grey area of automatic data collection. \n",
    "\n",
    "That's why before proceeding to scrape everything and anything, first you need to read the Terms and Conditions of a webpage, send them an email asking if what you're doing *really* is ok, or better yet - let professionals do the talking and outsource your data scraping needs, for example by using Find Data Lab.\n",
    "\n",
    "If you're still interested in the DIY version or just want to understand the principles of how data scraping works, follow along the (as of yet - still unfinished) journey of how I wanted to decipher what the ingredients on the back of my moisturisers label actually mean. \n",
    "\n",
    "Tasks for this project:\n",
    "1. Get a list of the ingredients from the web (done)\n",
    "<br>\n",
    "    1.1. Scrape\n",
    "<br>\n",
    "    1.2. Save the output as a .txt file or create a database\n",
    "2. Access PubChem chemistry database and get the short summaries of each compound/substance\n",
    "3. Read the output by candlelight while taking a bubble bath \n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is the pre-alpha version, we'll be trying to scrape the ingredients of a single product from only one brand.\n",
    "\n",
    "Headers is where you provide your user-agent and the referrer page. Depending on your goals and spiritual conviction, provide your contact info or just spoof the whole thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    #'user-agent' : \n",
    "    #'referer' : \n",
    "}\n",
    "urls = [\n",
    "        'https://www.sephora.com/brand/the-ordinary',\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The referrer page or \"referer\" might get checked as a part of a security thing, but it also might not. In any case, if you are worried about 'deep links' and the like, an example referer would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    #'user-agent' : \n",
    "    #'referer' : 'https://www.sephora.com/brand/'\n",
    "}\n",
    "urls = [\n",
    "        'https://www.sephora.com/brand/the-ordinary',\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same principle applies to every link. If for some reason this doesn't work, a country-specific google link could work as well.\n",
    "\n",
    "The next for loop is commented out - that would be the way to proceed if we'd have multiple brands to check out.\n",
    "The next lines of code load the html output of 'urls' and find the 'a' tag which defines a hyperlink. \n",
    "\n",
    "Every 'clickable' thing on a webpage is a hyperlink. That's something to keep in mind if your project involves navigating through multiple pages of a site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for n in range(len(urls)):\n",
    "n = 0\n",
    "page = requests.get(urls[n], headers = headers)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "subsections = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get the 'a' tags attribute 'href', which specifies the URL of the page the link goes to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for link in subsections:\n",
    "    links.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we've got all of the links. Since there's no need to navigate to \"Gifts\" or \"Quizzes\" we need to filter out the links that involve The Ordinary.\n",
    "\n",
    "By inspecting the product links, we can conclude that the unifying factor is the string \"theordinary_fromthebrand\", which means that in order to select all of the products individually we need to find a hyperlink with these words in it.\n",
    "\n",
    "This time, using regular expressions and a not-comprehended list, we loop through the list that contains *all* of the links and python will return a list containing either a NoneType object or a Match object. Match object is a link that contains the product identifying string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "reg = re.compile(\"theordinary_fromthebrand\")\n",
    "for n in range(len(links)):\n",
    "    products.append(reg.search(links[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we filter out all of the NoneType objects, which will leave us with a list containing only the product links.\n",
    "Let's use list comprehension. The structure is pretty simple and makes for an elegant solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list = [expression for item in iterable]\n",
    "f_products = [x for x in products if x is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finish link hunting by getting the Match objects string attribute and creating a full URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_products_links = [\"https://www.sephora.com\" + f_products[x].string for x in range(len(f_products))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's essentially it. Next we apply the same principles to get the actual list of product ingredients.\n",
    "\n",
    "As I was only interested in testing my method and *getting the answers*, I chose to pass the 7th link from the product list (remember that python lists start counting from 0, unlike e.g. R lists).\n",
    "\n",
    "Next we find the division that contains a specific class, which in our case is a text box on the website. \n",
    "\n",
    "You can find the specific identifiers by right-clicking anywhere on a webpage and choosing \"Inspect Element\" in Firefox or just \"Inspect\" in Google Chrome.\n",
    "\n",
    "Web scraping needs to be personalised for every website, since no page is the same. That's why the next lines are not exactly generalizable.\n",
    "\n",
    "Next we find the \"Ingredients\" tab in the output and select only the part that gets printed on the label. Here I found that it can be neatly done by splitting off a chunk of text, that follows two breaks.\n",
    "\n",
    "Then we clean up the text and create a numbered list that only contains the product ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads only the HA moisturising factors\n",
    "prod_page = requests.get(f_products_links[6])\n",
    "\n",
    "soup = BeautifulSoup(prod_page.content, 'html.parser')\n",
    "ingreds = soup.find_all('div', {'class': 'css-pz80c5'})\n",
    "title = soup.find('span', {'class': 'css-0'}).text\n",
    "\n",
    "#ingredients tab\n",
    "ing_tab = str(ingreds[2]).split('<br/><br/>')\n",
    "#get a clean string: ingredients separated by commas\n",
    "label = ing_tab[1]\n",
    "for repl in ['.', '</div>', '\\r', '\\n']:\n",
    "    if repl in label:\n",
    "        label = label.replace(repl, '')\n",
    "        \n",
    "label = label.split(', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step we get a neat list, where every ingredient is a separate list object.\n",
    "\n",
    "The last step is optional. Since I don't have the rest of my project done, I can't pass the list to another script.\n",
    "\n",
    "In any case, I think it's a good idea to have some backups of data you collect along the way (e.g. save links in another file etc.). There are many options for saving output and I'm going to choose the BEST one. \n",
    "That's a joke, I just like the aesthetic of a .txt file.\n",
    "\n",
    "So, if you want to save the output as a .txt file with every ingredient in a new line, use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/path/to/file/folder'\n",
    "\n",
    "#write all product ingredients in a .txt file\n",
    "#if you decide to spend your night becoming a rogue biochemist:\n",
    "#make all of the file names\n",
    "#a_bunch_of_files = [\"path/to/file%i.txt\" %x for x in range(len(f_products_links))]\n",
    "#loop the rest, or just make a database\n",
    "links_path = filepath + '/ingred_list.txt'\n",
    "with open(links_path, 'w+') as linksfile:\n",
    "    linksfile.write(title + '\\n') #the first line is the product name\n",
    "    for it in label:\n",
    "        linksfile.write('%s\\n' % it)\n",
    "    \n",
    "linksfile.close()\n",
    "linksfile.closed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is it! Part one of \"I wanted to decipher what the ingredients on the back of my moisturisers label actually mean\" is done and hopefully you've gotten a little insight into how web scraping works."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
