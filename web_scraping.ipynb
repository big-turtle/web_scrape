{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping: Almost Data Science With Python And BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping is a wonderful facet of programming which lets you collect incredible amounts of information automatically. Along with the freedom you get from not needing to sit in front of a computer for 6 hours, you're also faced with the legal grey area of automatic data collection. \n",
    "\n",
    "That's why before proceeding to scrape everything and anything, first you need to read the Terms and Conditions of a webpage, send them an email asking if what you're doing *really* is ok, or better yet - let professionals do the talking and outsource your data scraping needs, for example by using Find Data Lab.\n",
    "\n",
    "If you're still interested in the DIY version or just want to understand the principles of how data scraping works, follow along the (as of yet - still unfinished) journey of how I wanted to decipher what the ingredients on the back of my moisturisers label actually mean. \n",
    "\n",
    "Tasks for this project:\n",
    "1. Get a list of the ingredients from the web (done)\n",
    "    1.1. Scrape\n",
    "    1.2. Save the output as a .txt file or create a database\n",
    "2. Access PubChem chemistry database and get the short summaries of each compound/substance\n",
    "3. Read the output by candlelight while taking a bubble bath \n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is the pre-alpha version, we'll be trying to scrape the ingredients of a single product from only one brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "        'https://www.sephora.com/brand/the-ordinary',\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loop is commented out - that would be the way to proceed if we'd have multiple brands to check out.\n",
    "The next lines of code load the html output of 'urls' and find the <a> tag which defines a hyperlink. \n",
    "\n",
    "Every 'clickable' thing on a webpage is a hyperlink. That's something to keep in mind if your project involves navigating through multiple pages of a site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for n in range(len(urls)):\n",
    "n = 0\n",
    "page = requests.get(urls[n])\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "subsections = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get the <a> tags attribute href, which specifies the URL of the page the link goes to.\n",
    "    \n",
    "Some technicalities include initialising the list before looping through it and appending the list while in the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for link in subsections:\n",
    "    links.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we've got all of the links, but that's of no use, since there's no need to navigate to \"Gifts\" or \"Quizzes\".\n",
    "\n",
    "By inspecting the product links, we can conclude that the unifying factor is the string \"theordinary_fromthebrand\", which means that in order to select all of the products individually we need to find a hyperlink with these words in it.\n",
    "\n",
    "Using regular expressions we loop through a list containing all of the links and python returns a list containing either a NoneType object or a Match object. Match object is a link that contains the product identifying string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "reg = re.compile(\"theordinary_fromthebrand\")\n",
    "for n in range(len(links)):\n",
    "    products.append(reg.search(links[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we filter out all of the NoneType objects, which will leave us with a list containing only the product links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_products = [x for x in products if x is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finish link hunting by getting the Match objects string attribute and creating a full URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_products_links = [\"https://www.sephora.com\" + f_products[x].string for x in range(len(f_products))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's essentially it. Next we apply the same principles to get the actual list of product ingredients.\n",
    "\n",
    "As I was only interested in testing my method and *getting the answers*, I chose to pass the 7th link from the product list (remember that python lists start counting from 0, unlike e.g. R lists).\n",
    "\n",
    "Next we find the division that contains a specific class, which in our case is a text box on the website. \n",
    "\n",
    "You can find the specific identifiers by right-clicking anywhere on a webpage and choosing \"Inspect Element\" in Firefox or just \"Inspect\" in Google Chrome.\n",
    "\n",
    "Web scraping needs to be personalised for every website, since no page is the same. That's why the next lines are not exactly generalizable.\n",
    "\n",
    "Next we find the \"Ingredients\" tab in the output and select only the part that gets printed on the label. Here I found that it can be neatly done by splitting off a chunk of text, that follows two breaks.\n",
    "\n",
    "Then we clean up the text and create a numbered list that only contains the product ingredients. Using replace() four times is not very elegant, but hey, it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads only the HA moisturising factors\n",
    "prod_page = requests.get(f_products_links[6])\n",
    "\n",
    "soup = BeautifulSoup(prod_page.content, 'html.parser')\n",
    "ingreds = soup.find_all('div', {'class': 'css-pz80c5'})\n",
    "title = soup.find('span', {'class': 'css-0'}).text\n",
    "\n",
    "#ingredients tab\n",
    "ing_tab = str(ingreds[2]).split('<br/><br/>')\n",
    "#this is like the expressional equivalent of moon moon\n",
    "label = ing_tab[1].replace('.', '').replace('</div>', '').replace('\\r', '').replace('\\n', '').split(', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is optional. If you want to save the output as a .txt file with every ingredient in a new line, use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/path/to/file/folder'\n",
    "\n",
    "#write all product ingredients in a .txt file\n",
    "#if you decide to spend your night becoming a rogue biochemist:\n",
    "#make all of the file names\n",
    "#a_bunch_of_files = [\"path/to/file%i.txt\" %x for x in range(len(f_products_links))]\n",
    "#loop the rest, or just make a database\n",
    "links_path = filepath + '/ingred_list.txt'\n",
    "with open(links_path, 'w+') as linksfile:\n",
    "    linksfile.write(title + '\\n') #the first line is the product name\n",
    "    for it in label:\n",
    "        linksfile.write('%s\\n' % it)\n",
    "    \n",
    "linksfile.close()\n",
    "linksfile.closed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is it! Part one of \"I wanted to decipher what the ingredients on the back of my moisturisers label actually mean\" is done and hopefully you've gotten a little insight into how web scraping works.\n",
    "\n",
    "If you don't want to deal with the legal aspects of web scraping and spend a bunch of your valuable time, make your life easier and outsource your work to professionals. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
