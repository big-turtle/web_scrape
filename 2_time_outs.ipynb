{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-outs\n",
    "\n",
    "This article builds on the previous one. The following code will create a .txt file with one of the Ordinary product's ingredient list. Just like a fine-made anything, we shall refine the method!\n",
    "\n",
    "Full code from previous article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "#dict\n",
    "headers = {\n",
    "    'user-agent' : 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36',\n",
    "    'referer' : 'https://www.sephora.com/brand/'\n",
    "}\n",
    "urls = [\n",
    "        'https://www.sephora.com/brand/the-ordinary',\n",
    "        ]\n",
    "\n",
    "#for n in range(len(urls)):\n",
    "n = 0\n",
    "page = requests.get(urls[n], headers = headers)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#finds all of the urls\n",
    "subsections = soup.find_all('a')\n",
    "\n",
    "#makes a list of all urls\n",
    "links = []\n",
    "for link in subsections:\n",
    "    links.append(link.get('href'))\n",
    "\n",
    "#regex finds the ordinary links\n",
    "products = []\n",
    "reg = re.compile(\"theordinary_fromthebrand\")\n",
    "for n in range(len(links)):\n",
    "    products.append(reg.search(links[n]))\n",
    "    \n",
    "#filters out NoneType objects, only product links left\n",
    "f_products = [x for x in products if x is not None]\n",
    "\n",
    "f_products_links = [\"https://www.sephora.com\" + f_products[x].string for x in range(len(f_products))]\n",
    "\n",
    "#loads only the HA moisturising factors\n",
    "prod_page = requests.get(f_products_links[6])\n",
    "\n",
    "soup = BeautifulSoup(prod_page.content, 'html.parser')\n",
    "ingreds = soup.find_all('div', {'class': 'css-pz80c5'})\n",
    "title = soup.find('span', {'class': 'css-0'}).text\n",
    "\n",
    "#ingredients tab\n",
    "ing_tab = str(ingreds[2]).split('<br/><br/>')\n",
    "#haha lol i haven't thought of how to change this line yet, but it works\n",
    "label = ing_tab[1].replace('.', '').replace('</div>', '').replace('\\r', '').replace('\\n', '').split(', ')\n",
    "\n",
    "filepath = '/path/to/file/folder'\n",
    "\n",
    "#write all product ingredients in a .txt file\n",
    "#if you decide to spend your night becoming a rogue biochemist:\n",
    "#make all of the file names\n",
    "#a_bunch_of_files = [\"path/to/file%i.txt\" %x for x in range(len(f_products_links))]\n",
    "#loop the rest, or just make a database\n",
    "links_path = filepath + '/ingred_list.txt'\n",
    "with open(links_path, 'w+') as linksfile:\n",
    "    linksfile.write(title + '\\n') #the first line is the product name\n",
    "    for it in label:\n",
    "        linksfile.write('%s\\n' % it)\n",
    "    \n",
    "linksfile.close()\n",
    "linksfile.closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are incredibly smart and capable people, so we know that in order to not get banned, we need to do a few things:\n",
    "1. Time-out the requests\n",
    "2. Change the user-agents\n",
    "3. Change the IPs\n",
    "\n",
    "I am not condoning basically launching a sorta kinda DDoS attack, so use your brain and be nice to sysadmins. You might not have to do the 3rd point, you kind of need to do the 2nd, but you definitely need to do the 1st.\n",
    "\n",
    "So, basically we need to set some sort of a time-out to space out the requests sent to a web page.\n",
    "\n",
    "There are a few ways of timing-out the requests:\n",
    "1. Just time.sleep()\n",
    "2. Responsive delay\n",
    "3. Randomized responsive delay\n",
    "\n",
    "## Just time.sleep()\n",
    "To accomplish this, we'll need to import the Time module, and, honestly, the function is pretty self-explanatory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for n in urls:\n",
    "    page = requests.get(urls, headers = headers)\n",
    "    # the scrapening happens\n",
    "    time.sleep(5) #this is the time-out: 5 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responsive delay\n",
    "Depending on the time of day and web traffic, responsive delays might come in handy.\n",
    "\n",
    "The delay will be proportional to how long it took to load the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in urls:\n",
    "    start = time.time()\n",
    "    page = requests.get(urls[n], headers = headers)\n",
    "    delay = time.time() - start\n",
    "    # the scrapening\n",
    "    time.sleep(delay) #sleeps for however long it took to load the page"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time.time() returns a float - time in seconds since the epoch (platform dependent. For Unix it's January 1, 1970, 00:00:00 (UTC).\n",
    "> <font color='pink'> According to Wikipedia, the Unix time is sometimes referred to as *the Epoch* and I think that's beautiful. </font> \n",
    "\n",
    "Can I elaborate on this matter? No. And it's not necessary at this point, we can talk about it later.\n",
    "\n",
    "## Randomized responsive delay\n",
    "Now we get to the f-u-u-n stuff. The point of randomized responsive delays is to make 'em guess - your requests will be completely random **and** responsive to the site's abilities to handle traffic.\n",
    "People are random, so we need the script to be random. \n",
    "![Roll-Safe.jpg](https://3fybkfrr10x3tgp41p45lr3a-wpengine.netdna-ssl.com/wp-content/uploads/2017/03/Roll-Safe.jpg)\n",
    "\n",
    "Here's how with the python Random module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for n in urls:\n",
    "    start = time.time()\n",
    "    page = requests.get(urls[n], headers = headers)\n",
    "    delay = time.time() - start\n",
    "    # the scrapening\n",
    "    time.sleep(random.uniform(1, 2) * delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the script will time out sometime between 1 and 2 times longer than it took to load the site.\n",
    "\n",
    "This is the (for now) finished code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "#dict\n",
    "headers = {\n",
    "    'user-agent' : 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36',\n",
    "    'referer' : 'https://www.sephora.com/brand/'\n",
    "}\n",
    "urls = [\n",
    "        'https://www.sephora.com/brand/the-ordinary',\n",
    "        ]\n",
    "\n",
    "#for n in range(len(urls)):\n",
    "n = 0\n",
    "start = time.time()\n",
    "page = requests.get(urls[n], headers = headers)\n",
    "delay = time.time() - start\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#finds all of the urls\n",
    "subsections = soup.find_all('a')\n",
    "\n",
    "#makes a list of all urls\n",
    "links = []\n",
    "for link in subsections:\n",
    "    links.append(link.get('href'))\n",
    "\n",
    "#regex finds the ordinary links\n",
    "products = []\n",
    "reg = re.compile(\"theordinary_fromthebrand\")\n",
    "for n in range(len(links)):\n",
    "    products.append(reg.search(links[n]))\n",
    "    \n",
    "#filters out NoneType objects, only product links left\n",
    "f_products = [x for x in products if x is not None]\n",
    "\n",
    "f_products_links = [\"https://www.sephora.com\" + f_products[x].string for x in range(len(f_products))]\n",
    "\n",
    "#loads only the HA moisturising factors\n",
    "prod_page = requests.get(f_products_links[6])\n",
    "\n",
    "soup = BeautifulSoup(prod_page.content, 'html.parser')\n",
    "ingreds = soup.find_all('div', {'class': 'css-pz80c5'})\n",
    "title = soup.find('span', {'class': 'css-0'}).text\n",
    "\n",
    "#ingredients tab\n",
    "ing_tab = str(ingreds[2]).split('<br/><br/>')\n",
    "#haha lol i haven't thought of how to change this line yet, but it works\n",
    "label = ing_tab[1].replace('.', '').replace('</div>', '').replace('\\r', '').replace('\\n', '').split(', ')\n",
    "\n",
    "filepath = '/path/to/file/folder'\n",
    "\n",
    "#write all product ingredients in a .txt file\n",
    "#if you decide to spend your night becoming a rogue biochemist:\n",
    "#make all of the file names\n",
    "#a_bunch_of_files = [\"path/to/file%i.txt\" %x for x in range(len(f_products_links))]\n",
    "#loop the rest, or just make a database\n",
    "links_path = print(filepath + '/ingred_list%i.txt' %n)\n",
    "with open(links_path, 'w+') as linksfile:\n",
    "    linksfile.write(title + '\\n') #the first line is the product name\n",
    "    for it in label:\n",
    "        linksfile.write('%s\\n' % it)\n",
    "    \n",
    "linksfile.close()\n",
    "linksfile.closed\n",
    "\n",
    "time.sleep(random.uniform(1, 2) * delay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
